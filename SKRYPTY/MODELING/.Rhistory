library(readxl)
library(readr)
library(dplyr)
library(magrittr)
library(ggplot2)
library(e1071)
library(stats)
library(plumber)
library(readr)
library(crosstalk)
library(knitr)
library(shinyWidgets)
library(plotly)
library(shiny)
library(ggplot2)
library(DT)
library(shinythemes)
library(rmarkdown)
library(tidyr)
library(shinyauthr)
library(shinyjs)
library(shinyalert)
library(DALEX)
#library(h2o)
#library(keras)
library(chron)
library(onewaytests)
library(breakDown)
library(Boruta)
library(formattable)
library(rjson)
library(reticulate)
library(shinydashboard)
library(stringr)
library(gamlss)
library(fitdistrplus)
library(dbscan)
library(shinyWidgets)
library(locfit)
library(extRemes)
library(shinydashboard)
library(imager)
library(caret)
library(RODBC)
library(lubridate)
library(readxlsb)
library(devtools)
library(gtools)
#devtools::install_github("rcannood/gng")
library(corrplot)
library(dashboardthemes)
library(rpart)
library(rpart.plot)
library(RPostgreSQL)
library(naniar)
set.seed(100)
#### Welcome to Predictive QC workfile ####
# You will use this file to call functions related to AI_assistant, variable importance and model performance
# First, make sure that all of the libraries from ai_assistant/Scripts are required
### Part 1 - load the functions
your_dir = "C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/ai_asistant/Scripts/"
#your_dir=paste('C:/Users/',Sys.getenv("USERNAME"),'/Desktop/ai_asistant/Scripts/',sep = '')
basic_plots=dget(paste(your_dir,"basic_plots.R",sep=''))
basic_statistics_exploratory_analysis=dget(paste(your_dir,"basic_statistics_exploratory_analysis.R",sep=''))
correlations_exploratory_analysis=dget(paste(your_dir,"correlations_exploratory_analysis.R",sep=''))
expected_freq_exploratory_analysis=dget(paste(your_dir,"expected_freq_exploratory_analysis.R",sep=''))
histogram_exploratory_analysis=dget(paste(your_dir,"histogram_exploratory_analysis.R",sep=''))
boxplot_exploratory_analysis=dget(paste(your_dir,"boxplot_exploratory_analysis.R",sep=''))
plotting_freq_exploratory_analysis=dget(paste(your_dir,"plotting_freq_exploratory_analysis.R",sep=''))
basic_plots=dget(paste(your_dir,"basic_plots.R",sep=''))
selected_features_table = dget(paste(your_dir,"selected_features_table.R",sep=''))
feature_importance_table = dget(paste(your_dir,"feature_importance_table.R",sep=''))
plot_model_performance = dget(paste(your_dir,"plot_model_performance.R",sep=''))
quantiles_and_IQR = dget(paste(your_dir,"quantiles_and_IQR.R",sep=''))
barchart_classes = dget(paste(your_dir,"barchart_classes.R",sep=''))
model_vs_time = dget(paste(your_dir,"model_vs_time.R",sep=''))
histogram_error_ratio_model_performance = dget(paste(your_dir,"histogram_error_ratio_model_performance.R",sep=''))
histogram_box_avg_score =   dget(paste(your_dir,"histogram_box_avg_score.R",sep=''))
CalculateAUC =   dget(paste(your_dir,"CalculateAUC.R",sep=''))
fastAUC = dget(paste(your_dir,"fastAUC.R",sep=''))
data_preprocessing = dget(paste(your_dir,"data_preprocessing.R",sep=''))
create_test_train = dget(paste(your_dir,"create_test_train.R",sep=''))
Lasso_feature_selection =  dget(paste(your_dir,"Lasso_feature_selection.R",sep=''))
Boruta_feature_selection = dget(paste(your_dir,"Boruta_feature_selection.R",sep=''))
RF_feature_selection =  dget(paste(your_dir,"RF_feature_selection.R",sep=''))
AI_Assistant = dget("C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_db/scripts/AI_Assistant_DB.R")
# Part 2 - load dataset
# Dataset should be already prepared in python/sql script and ready to visualize it in AI_assistant
# Make sure that dataset has features "Label" and "datestamp"
### change path to where you store iqa data
dataset = read.csv("C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_db/data/processed/incremented/cib_pc_15052020.txt",
sep = '|')
colnames(dataset)[1]='Unique'
dataset$Label = ifelse(dataset$Score==100,0,1)
#dataset = dataset[dataset$error_binary_6 != 'NULL',]
#dataset$Label = ifelse(dataset$error_binary_6 == 1, 1, 0)
dataset = as.data.frame(dataset)
toMatch <- c("Outreach_", "score_deducted", "error_ratio", "analyst_cases_average_score", "analyst_score_deducted",
"analyst_error_ratio", "analyst_expierence_day", "qc_cases_average_score", "qc_score_deducted",
"qc_error_ratio", "qc_expierence_day")
ratioMatch <- c("error_ratio", "analyst_error_ratio", "qc_error_ratio")
cols_to_transform <- unique(grep(paste(toMatch, collapse="|"), colnames(dataset), value=TRUE))
ratio_cols <- unique(grep(paste(ratioMatch, collapse="|"), colnames(dataset), value=TRUE))
dataset[cols_to_transform][dataset[cols_to_transform] == "NULL"] <- NA
dataset[cols_to_transform] <- lapply(dataset[cols_to_transform], function(x) gsub(",", ".", x))
dataset[cols_to_transform] <- lapply(dataset[cols_to_transform], function(x) as.numeric(x))
dataset$datestamp = as.POSIXct(as.character(dataset$checklist_submission), format = '%Y-%m-%d')
# Add Experience stats
add_experience = function(dataset, person = 'Analyst'){
exp = dataset %>%
dplyr :: group_by(get(person)) %>%
summarize(start_date = min(datestamp))
helper = merge(data.frame(dataset[,c(person,'datestamp')]),
exp,
by.x=person,
by.y = 'get(person)')
helper$expierence_day = helper[,2]-helper[,3]
units(helper$expierence_day) = 'days'
helper$expierence_day = as.numeric(helper$expierence_day)
colname = paste(tolower(person), '_expierence_day',sep='')
dataset[,colname] = helper$expierence_day
return(dataset)
}
dataset = add_experience(dataset,
person = 'Analyst')
# Remove first cases for each analyst
dataset = dataset[dataset$analyst_cases_processed_all>0,]
# Remove unnesesary columns :
dataset_feature_selection = dataset
unused_columns = c("Stage", "Analyst", "QC",
"Major", "Minor", "Admin", "checklist_submission", "dbkycs_status",
"Outreach_Loop_Counts", "Outreach_Onfile", "Outreach_Pending", "Outreach_Provided", "Outreach_Reject",
"Screening_Results_AFC_MI", "Score",
"score_deducted_1", "score_deducted_2", "score_deducted_3", "score_deducted_4", "score_deducted_5",
"score_deducted_6", "score_deducted_7", "score_deducted_8", "score_deducted_9", "score_deducted_10",
"score_deducted_11", "score_deducted_12", "score_deducted_13", "score_deducted_14", "score_deducted_15",
"score_deducted_17", "score_deducted_18", "analyst_cases_average_score_30days",
"analyst_cases_average_score_90days", "qc_cases_average_score_30days",
"qc_cases_processed_30days", "qc_cases_processed_90days", "qc_cases_processed_all",
"qc_cases_average_score_90days", "qc_cases_average_score_all", "qc_expierence_day",
"qc_score_deducted_1_all", "qc_score_deducted_2_all", "qc_score_deducted_3_all",
"qc_score_deducted_4_all", "qc_score_deducted_5_all", "qc_score_deducted_6_all",
"qc_score_deducted_7_all", "qc_score_deducted_8_all", "qc_score_deducted_9_all",
"qc_score_deducted_10_all", "qc_score_deducted_11_all", "qc_score_deducted_12_all",
"qc_score_deducted_13_all", "qc_score_deducted_14_all", "qc_score_deducted_15_all",
"qc_score_deducted_17_all", "qc_score_deducted_18_all",
"qc_error_binary_1_all", "qc_error_binary_2_all", "qc_error_binary_3_all",
"qc_error_binary_4_all", "qc_error_binary_5_all", "qc_error_binary_6_all",
"qc_error_binary_7_all", "qc_error_binary_8_all", "qc_error_binary_9_all",
"qc_error_binary_10_all", "qc_error_binary_11_all", "qc_error_binary_12_all",
"qc_error_binary_13_all", "qc_error_binary_14_all", "qc_error_binary_15_all",
"qc_error_binary_17_all", "qc_error_binary_18_all"
)
dataset_feature_selection=dataset_feature_selection[,-which(
colnames(dataset_feature_selection)%in%unused_columns)]
helper_1 = as.data.frame(prop.table(table(dataset$Entity)))
dataset_feature_selection$Entity=factor(
ifelse(as.character(dataset_feature_selection$Entity)%in%helper_1$Var1[helper_1$Freq>0.01],
as.character(dataset_feature_selection$Entity),'Other_entity'))
helper_2 = as.data.frame(prop.table(table(dataset$High_Value_Client)))
dataset_feature_selection$High_Value_Client=factor(
ifelse(as.character(dataset_feature_selection$High_Value_Client)%in%helper_2$Var1[helper_2$Freq>0.01],
as.character(dataset_feature_selection$High_Value_Client),'Other_entity'))
Labels_category = c(
"error_binary_1", "error_binary_2", "error_binary_3",
"error_binary_4", "error_binary_5", "error_binary_6",
"error_binary_7", "error_binary_8", "error_binary_9",
"error_binary_10", "error_binary_11", "error_binary_12",
"error_binary_13", "error_binary_14", "error_binary_15",
"error_binary_17", "error_binary_18", "Label"
)
for(i in 6:(length(dataset_feature_selection)-1)){
dataset_feature_selection[,i] =
as.numeric(as.character(dataset_feature_selection[,i]))
}
### specify the label of valid category to build model
dataset_feature_selection_cat1 = dataset_feature_selection[,-which(
colnames(dataset_feature_selection)%in%Labels_category[-8])]
colnames(dataset_feature_selection_cat1)[which(
colnames(dataset_feature_selection_cat1)=='error_binary_8')]="Label"
### remove rest of the columns with missing values
dataset_feature_selection_cat1 =
dataset_feature_selection_cat1[ , colSums(is.na(dataset_feature_selection_cat1)) < 200]
### Part 5 - dummy the data
dataset_dummy_cat1 = data_preprocessing(dataset_feature_selection_cat1,
frozen_columns = c('Unique', 'datestamp', "Label" ),
remove_na = T)
library(solitude)
library(cclust)
library(RColorBrewer)
library(RCurl)
library(kohonen)
library(sqldf)
library(DMwR)
library(igraph)
library(mlr)
library(polycor)
library(BBmisc)
library(xlsx)
library(readxl)
library(readr)
library(dplyr)
library(magrittr)
library(ggplot2)
library(e1071)
library(stats)
library(plumber)
library(readr)
library(crosstalk)
library(knitr)
library(shinyWidgets)
library(plotly)
library(shiny)
library(ggplot2)
library(DT)
library(shinythemes)
library(rmarkdown)
library(tidyr)
library(shinyauthr)
library(shinyjs)
library(shinyalert)
library(DALEX)
#library(h2o)
#library(keras)
library(chron)
library(onewaytests)
library(breakDown)
library(Boruta)
library(formattable)
library(rjson)
library(reticulate)
library(shinydashboard)
library(stringr)
library(gamlss)
library(fitdistrplus)
library(dbscan)
library(shinyWidgets)
library(locfit)
library(extRemes)
library(shinydashboard)
library(imager)
library(caret)
library(RODBC)
library(lubridate)
library(readxlsb)
library(devtools)
library(gtools)
#devtools::install_github("rcannood/gng")
library(corrplot)
library(dashboardthemes)
library(rpart)
library(rpart.plot)
library(RPostgreSQL)
library(naniar)
set.seed(100)
#### Welcome to Predictive QC workfile ####
# You will use this file to call functions related to AI_assistant, variable importance and model performance
# First, make sure that all of the libraries from ai_assistant/Scripts are required
### Part 1 - load the functions
your_dir = "C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/ai_asistant/Scripts/"
#your_dir=paste('C:/Users/',Sys.getenv("USERNAME"),'/Desktop/ai_asistant/Scripts/',sep = '')
basic_plots=dget(paste(your_dir,"basic_plots.R",sep=''))
basic_statistics_exploratory_analysis=dget(paste(your_dir,"basic_statistics_exploratory_analysis.R",sep=''))
correlations_exploratory_analysis=dget(paste(your_dir,"correlations_exploratory_analysis.R",sep=''))
expected_freq_exploratory_analysis=dget(paste(your_dir,"expected_freq_exploratory_analysis.R",sep=''))
histogram_exploratory_analysis=dget(paste(your_dir,"histogram_exploratory_analysis.R",sep=''))
boxplot_exploratory_analysis=dget(paste(your_dir,"boxplot_exploratory_analysis.R",sep=''))
plotting_freq_exploratory_analysis=dget(paste(your_dir,"plotting_freq_exploratory_analysis.R",sep=''))
basic_plots=dget(paste(your_dir,"basic_plots.R",sep=''))
selected_features_table = dget(paste(your_dir,"selected_features_table.R",sep=''))
feature_importance_table = dget(paste(your_dir,"feature_importance_table.R",sep=''))
plot_model_performance = dget(paste(your_dir,"plot_model_performance.R",sep=''))
quantiles_and_IQR = dget(paste(your_dir,"quantiles_and_IQR.R",sep=''))
barchart_classes = dget(paste(your_dir,"barchart_classes.R",sep=''))
model_vs_time = dget(paste(your_dir,"model_vs_time.R",sep=''))
histogram_error_ratio_model_performance = dget(paste(your_dir,"histogram_error_ratio_model_performance.R",sep=''))
histogram_box_avg_score =   dget(paste(your_dir,"histogram_box_avg_score.R",sep=''))
CalculateAUC =   dget(paste(your_dir,"CalculateAUC.R",sep=''))
fastAUC = dget(paste(your_dir,"fastAUC.R",sep=''))
data_preprocessing = dget(paste(your_dir,"data_preprocessing.R",sep=''))
create_test_train = dget(paste(your_dir,"create_test_train.R",sep=''))
Lasso_feature_selection =  dget(paste(your_dir,"Lasso_feature_selection.R",sep=''))
Boruta_feature_selection = dget(paste(your_dir,"Boruta_feature_selection.R",sep=''))
RF_feature_selection =  dget(paste(your_dir,"RF_feature_selection.R",sep=''))
AI_Assistant = dget("C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_db/scripts/AI_Assistant_DB.R")
### change path to where you store iqa data
dataset = read.csv("C:\Users\mmandziej001\Desktop\FCU\SCRIPTS\predictive_qc_lion_king\data_prep\redevelopment data\QC_WF_HT_data.txt",
sep = '|')
### change path to where you store iqa data
#dataset = read.csv("C:\Users\mmandziej001\Desktop\FCU\SCRIPTS\predictive_qc_lion_king\data_prep\redevelopment data\QC_WF_HT_data.txt",
#                   sep = '|')
dataset = read_excel("C:\Users\mmandziej001\Desktop\FCU\SCRIPTS\predictive_qc_lion_king\data_prep\redevelopment data\QC_WF_HT_data.xlsx")
### change path to where you store iqa data
dataset = read.csv("C:/Users/mmandziej001/Desktop/FCU\SCRIPTS/predictive_qc_lion_king/data_prep/redevelopment data/QC_WF_HT_data/QC_WF_HT_data.txt",
sep = '|')
### change path to where you store iqa data
dataset = read.csv("C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_lion_king/data_prep/redevelopment data/QC_WF_HT_data/QC_WF_HT_data.txt",
sep = '|')
dataset = read_excel("C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_lion_king/data_prep/redevelopment data/QC_WF_HT_data.xlsx")
View(dataset)
dataset = as.data.frame(dataset)
dataset$datestamp = as.POSIXct(as.character(dataset$checklist_submission), format = '%Y-%m-%d')
dataset$datestamp = as.POSIXct(as.character(dataset$datestamp), format = '%Y-%m-%d')
dataset = read_excel("C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_lion_king/data_prep/redevelopment data/QC_WF_HT_data.xlsx")
View(dataset)
colnames(dataset)[1]='Unique'
dataset = as.data.frame(dataset)
dataset$datestamp = as.POSIXct(as.character(dataset$datestamp), format = '%Y-%m-%d')
### Part 3 - visualization of features
AI_Assistant(dataset = dataset, stage = 'Exploratory_analysis')
View(dataset)
# Remove unnesesary columns :
dataset_feature_selection = dataset
unused_columns = c('Created', 'Critial', 'Major', 'Minor', 'Score',
'AnalystAssignedName', 'QCAssignedName', 'ProjectJoiningDate', 'TeamJoiningDate'
)
dataset_feature_selection=dataset_feature_selection[,-which(
colnames(dataset_feature_selection)%in%unused_columns)]
Labels_category = c("Label")
dataset_feature_selection_cat1 = dataset_feature_selection[,-which(
colnames(dataset_feature_selection)%in%Labels_category[1])]
View(dataset)
# Remove unnesesary columns :
dataset_feature_selection = dataset
unused_columns = c('Created', 'Critial', 'Major', 'Minor', 'Score',
'AnalystAssignedName', 'QCAssignedName', 'ProjectJoiningDate', 'TeamJoiningDate'
)
dataset_feature_selection=dataset_feature_selection[,-which(
colnames(dataset_feature_selection)%in%unused_columns)]
### remove rest of the columns with missing values
dataset_feature_selection = dataset_feature_selection[ , colSums(is.na(dataset_feature_selection)) < 150]
# Remove unnesesary columns :
dataset_feature_selection = dataset
unused_columns = c('Created', 'Critial', 'Major', 'Minor', 'Score',
'AnalystAssignedName', 'QCAssignedName', 'ProjectJoiningDate', 'TeamJoiningDate'
)
dataset_feature_selection=dataset_feature_selection[,-which(
colnames(dataset_feature_selection)%in%unused_columns)]
rm(dataset_feature_selection_cat1)
# Remove unnesesary columns :
dataset_feature_selection = dataset
unused_columns = c('Created', 'Critial', 'Major', 'Minor', 'Score',
'AnalystAssignedName', 'QCAssignedName', 'ProjectJoiningDate', 'TeamJoiningDate'
)
dataset_feature_selection=dataset_feature_selection[,-which(
colnames(dataset_feature_selection)%in%unused_columns)]
helper_1 = as.data.frame(prop.table(table(dataset$Entity)))
### keep only obs with valid label
dataset_feature_selection = dataset_feature_selection[!is.na(dataset_feature_selection$Label),]
### remove rest of the columns with missing values
dataset_feature_selection = dataset_feature_selection[ , colSums(is.na(dataset_feature_selection)) < 300]
### Part 5 - dummy the data
dataset_dummy = data_preprocessing(dataset_feature_selection,
frozen_columns = c('Unique', 'datestamp', "Label" ),
remove_na = T)
library(solitude)
library(cclust)
library(RColorBrewer)
library(RCurl)
library(kohonen)
library(sqldf)
library(DMwR)
library(igraph)
library(mlr)
library(polycor)
library(BBmisc)
library(xlsx)
library(readxl)
library(readr)
library(dplyr)
library(magrittr)
library(ggplot2)
library(e1071)
library(stats)
library(plumber)
library(readr)
library(crosstalk)
library(knitr)
library(shinyWidgets)
library(plotly)
library(shiny)
library(ggplot2)
library(DT)
library(shinythemes)
library(rmarkdown)
library(tidyr)
library(shinyauthr)
library(shinyjs)
library(shinyalert)
library(DALEX)
#library(h2o)
#library(keras)
library(chron)
library(onewaytests)
library(breakDown)
library(Boruta)
library(formattable)
library(rjson)
library(reticulate)
library(shinydashboard)
library(stringr)
library(gamlss)
library(fitdistrplus)
library(dbscan)
library(shinyWidgets)
library(locfit)
library(extRemes)
library(shinydashboard)
library(imager)
library(caret)
library(RODBC)
library(lubridate)
library(readxlsb)
library(devtools)
library(gtools)
#devtools::install_github("rcannood/gng")
library(corrplot)
library(dashboardthemes)
library(rpart)
library(rpart.plot)
library(RPostgreSQL)
library(naniar)
set.seed(100)
### Part 1 - load the functions
your_dir = "C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/ai_asistant/Scripts/"
#your_dir=paste('C:/Users/',Sys.getenv("USERNAME"),'/Desktop/ai_asistant/Scripts/',sep = '')
basic_plots = dget(paste(your_dir,"basic_plots.R",sep=''))
basic_statistics_exploratory_analysis = dget(paste(your_dir,"basic_statistics_exploratory_analysis.R", sep=''))
correlations_exploratory_analysis = dget(paste(your_dir,"correlations_exploratory_analysis.R", sep=''))
expected_freq_exploratory_analysis = dget(paste(your_dir,"expected_freq_exploratory_analysis.R", sep=''))
histogram_exploratory_analysis = dget(paste(your_dir,"histogram_exploratory_analysis.R", sep=''))
boxplot_exploratory_analysis = dget(paste(your_dir,"boxplot_exploratory_analysis.R", sep=''))
plotting_freq_exploratory_analysis = dget(paste(your_dir,"plotting_freq_exploratory_analysis.R", sep=''))
basic_plots = dget(paste(your_dir,"basic_plots.R", sep=''))
selected_features_table = dget(paste(your_dir,"selected_features_table.R", sep=''))
feature_importance_table = dget(paste(your_dir,"feature_importance_table.R", sep=''))
plot_model_performance = dget(paste(your_dir,"plot_model_performance.R", sep=''))
quantiles_and_IQR = dget(paste(your_dir,"quantiles_and_IQR.R", sep=''))
barchart_classes = dget(paste(your_dir,"barchart_classes.R", sep=''))
model_vs_time = dget(paste(your_dir,"model_vs_time.R", sep=''))
histogram_error_ratio_model_performance = dget(paste(your_dir,"histogram_error_ratio_model_performance.R", sep=''))
histogram_box_avg_score = dget(paste(your_dir,"histogram_box_avg_score.R", sep=''))
CalculateAUC = dget(paste(your_dir,"CalculateAUC.R", sep=''))
fastAUC = dget(paste(your_dir,"fastAUC.R", sep=''))
data_preprocessing = dget(paste(your_dir,"data_preprocessing.R", sep=''))
create_test_train = dget(paste(your_dir,"create_test_train.R", sep=''))
Lasso_feature_selection = dget(paste(your_dir,"Lasso_feature_selection.R", sep=''))
Boruta_feature_selection = dget(paste(your_dir,"Boruta_feature_selection.R", sep=''))
RF_feature_selection =  dget(paste(your_dir,"RF_feature_selection.R", sep=''))
AI_Assistant = dget("C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_lion_king/model_training/scripts/AI_Assistant_LK.R")
dataset_dummy = read_excel('model_training/data/RAW/dataset_undersampled_inputed_normalized_no_vat.xlsx')
### save results after preprocessing of full dataset
setwd("C:/Users/mmandziej001/Desktop/Projects/FAIT/Prediction Module/POLAND_DANE/MODELING/")
dataset_dummy = read_excel('model_training/data/RAW/dataset_undersampled_inputed_normalized_no_vat.xlsx')
colnames(dataset_dummy)[1]='Unique'
dataset_dummy$datestamp = as.POSIXct(as.character(dataset_dummy$DataUpadlosci), format = '%Y-%m-%d')
unused_columns = c('DataUpadlosci',
'EntityListedInVATRegistry_NIE', 'EntityListedInVATRegistry_TAK',
'RiskyRemovalBasis_BrakDanych', 'RiskyRemovalBasis_LikelyFraudulent', 'RiskyRemovalBasis_NaturalReason', 'RiskyRemovalBasis_NeverRemoved',
'MainProductsNull_NIE', 'MainProductsNull_TAK',
'SegmentName_Not listed', 'SegmentName_Other')
dataset_dummy = dataset_dummy[, -which(
colnames(dataset_dummy) %in% unused_columns)]
### Part 6 - variable importance
RF_variable_importance = RF_feature_selection(dataset_dummy[,-which(
colnames(dataset_dummy)=='datestamp')])
Lasso_variable_importance = Lasso_feature_selection(dataset_dummy[,-which(
colnames(dataset_dummy)=='datestamp')],
label_column = 'Label')
Boruta_variable_importance = Boruta_feature_selection(dataset_dummy[,-which(
colnames(dataset_dummy)=='datestamp')],
label_column = 'Label')
Variable_importance_all_methods = list(Lasso_variable_importance, Boruta_variable_importance, RF_variable_importance)
colnames(dataset_dummy)[1]='Unique'
dataset_dummy$datestamp = as.POSIXct(as.character(dataset_dummy$DataUpadlosci), format = '%Y-%m-%d')
colnames(dataset_dummy)
dataset_dummy = read_excel('model_training/data/RAW/dataset_undersampled_inputed_normalized_no_vat.xlsx')
colnames(dataset_dummy)[1]='Unique'
dataset_dummy$datestamp = as.POSIXct(as.character(dataset_dummy$DataUpadlosci), format = '%Y-%m-%d')
unused_columns = c('DataUpadlosci',
'EntityListedInVATRegistry_NIE', 'EntityListedInVATRegistry_TAK',
'RiskyRemovalBasis_BrakDanych', 'RiskyRemovalBasis_LikelyFraudulent', 'RiskyRemovalBasis_NaturalReason', 'RiskyRemovalBasis_NeverRemoved',
'MainProductsNull_NIE', 'MainProductsNull_TAK',
'SegmentName_Not listed', 'SegmentName_Other')
dataset_dummy = dataset_dummy[, -which(
colnames(dataset_dummy) %in% unused_columns)]
### Part 6 - variable importance
RF_variable_importance = RF_feature_selection(dataset_dummy[,-which(
colnames(dataset_dummy)=='datestamp')])
Lasso_variable_importance = Lasso_feature_selection(dataset_dummy[,-which(
colnames(dataset_dummy)=='datestamp')],
label_column = 'Label')
Boruta_variable_importance = Boruta_feature_selection(dataset_dummy[,-which(
colnames(dataset_dummy)=='datestamp')],
label_column = 'Label')
Variable_importance_all_methods = list(Lasso_variable_importance, Boruta_variable_importance, RF_variable_importance)
save(list = ls(all.names = TRUE),
file = "R_DUMPS/feature_selection.RData")
write.csv(Lasso_variable_importance, "R_DUMPS/Lasso_variable_importance_inputed.csv", row.names = F)
write.csv(Boruta_variable_importance, "R_DUMPS/Boruta_variable_importance_inputed.csv", row.names = F)
write.csv(RF_variable_importance, "R_DUMPS/RF_variable_importance_inputed.csv", row.names = F)
save(list = ls(all.names = TRUE),
file = "model_training/R_DUMPS/feature_selection_inputed.RData")
write.csv(Lasso_variable_importance, "model_training/R_DUMPS/Lasso_variable_importance_inputed.csv", row.names = F)
write.csv(Boruta_variable_importance, "model_training/R_DUMPS/Boruta_variable_importance_inputed.csv", row.names = F)
write.csv(RF_variable_importance, "model_training/R_DUMPS/RF_variable_importance_inputed.csv", row.names = F)
### Part 7 - AI Assistant with feature importance
AI_Assistant(dataset = dataset,
stage = 'Feature_selection',
Variable_importance_all_methods = Variable_importance_all_methods,
colnames_feature_selection = colnames(dataset_dummy)[-which(
colnames(dataset_dummy)=='datestamp')])
